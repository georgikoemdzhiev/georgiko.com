<div class="d-flex flex-column mb-3">
    <h1 class="p-2 font-weight-bold">
        Database Replication from Microsoft SQL Server to PostgreSQL on AWS using Python and EC2
        <small class="text-muted">Jan 10, 2021</small>
    </h1>

    <p class="p-2">Imagine the following scenario. You and your are responsible for a web app which uses data from
        a table in a Microsoft SQL Server database managed by another team. Let's call that team - "<i>DB team</i>".
        Every day, the DB team
        runs an automated script which updates the database table using a <i>Kill And Fill</i> strategy. That means
        they delete all of the data in the table we use and start filling it with "fresh" data.

        The problem for our app is that if a user visits our website while the update script is running it
        resulds in a lot of <i>Deadlock</i> exceptions (see this <a
            href="https://vladmihalcea.com/database-deadlock/">post</a> for more
        detailed explanation of what Deadlock is) as we are trying to read records from a table that was getting
        updated at the same time.
    </p>

    <h3>The problem</h3>
    <p>
        You are proobably thinking <i>"You should just talk to the DB team and explain the problem so they can change
            the way their script works"</i>. The problem is that we tried to comunicate the problem on numerous
        occations but was apparant that the DB
        team was unwilling to change how their script works. It was apparant for our team that we needed to take the
        things in our hands as the current design was simply unexceptable for a
        <b>production</b> system.
    </p>

    <h3>Solution</h3>
    <p>
        It was becoming clear for our team that we needed to replicate the table we use from the "source" table to our
        own database so we have
        full controll over it and update the data in a smarter way from the "upstream" table from the DB team.
    </p>

    <p>
        After a few meetings we extablished the following requirements of the system:
    <ol>
        <li>It should perform both manual and scheduled replication from the source table to our own table in the
            PostgreSQL</li>
        <li>The system should be inexpensive to operate</li>
        <li>The web app should not be affected in any way while the table data is updated from upstream table</li>
    </ol>

    Our team came up with this system design:
    <br>
    <br>

    <img width="900px" src="/assets/posts/2/system_design.png" alt="System design diagram">
    </p>

    <p>
        The main idea behind the architecture of the the system is to have a EC2 instance which executes a python script
        at boot time
        that does the data replication
        and when that process is done, the script terminates the instance (i.e. self-terminate). The EC2 instance is
        created/launched by a lambda
        function. We chouse lambda as it allwed us to satisfy the first requirement - the function can be triggered
        manually
        (i.e.
        AWS Console) or on a scheduled basis using
        <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html">CloudWatch Scheduled
            events</a>.

        <br> By using Lambda and terminating the EC2 instance we satisfied the second requirement as we only pay for the
        computing time. That applies to Lambda as well as the EC2 instance as it get's terminated when it finishes its
        work. <br>
        The last requirement is satisfied by the use of 2 database tables created in our Aurora Serverless database. We
        use <a href="https://en.wikipedia.org/wiki/Blue-green_deployment">BLUE/GREEN deployment</a> strategy which
        allows us to perform a sync into a table while our customers are using another table. At the end of the sync we
        swap the "current" table (i.e. the table that is used by our app/customers) with the one that we performed the
        sync
        to.
    </p>

    <h3>Python script</h3>
    <p>
        The workflow of the script can be seen in the following diagram:
        <br>
        <img height="500px" width="600px" src="/assets/posts/2/script_workflow.png" alt="Python script diagram">

        <br>
        The script starts with connecting to the source database (in our case that is a MSSQL Server) and performs an
        export
        of a table to a csv file on the EC2 instance. If that is successful, we query DynamoDB table that stores 1 item
        and
        that is the name of the currently used table, for example "TABLE_GREEN". We delete all of the records of the
        "TABLE_BLUE" table and start updating it with the data from the csv file.
        When that is finished, we recreate the <b>view</b> using "TABLE_BLUE" table and set
        the "current name" to be "TABLE_BLUE" as well and shutdown the EC2 instance.

        <br>
    <pre><code>&lt;p&gt;Sample code here...&lt;/p&gt;
            &lt;p&gt;And another line of sample text here...&lt;/p&gt;
            </code></pre>

    </p>
</div>