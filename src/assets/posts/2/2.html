<div class="d-flex flex-column mb-3">
    <h1 class="p-2 font-weight-bold">
        Database Replication System on AWS using Python script on EC2 instance
        <small class="text-muted">Apr 2, 2021</small>
    </h1>

    <p class="p-2">Imagine the following scenario. You and your are responsible for a web app which uses data from
        a table in a Microsoft SQL Server database managed by another team. Let's call that team - "<i>DB team</i>".
        Every day, the DB team
        runs an automated script which updates the database table using a <i>Kill And Fill</i> strategy. That means
        they delete all of the data in the table we use and start filling it with "fresh" data.

        The problem for our app is that if a user visits our website while the update script is running it
        resulds in a lot of <i>Deadlock</i> exceptions (see this <a
            href="https://vladmihalcea.com/database-deadlock/">post</a> for more
        detailed explanation of what Deadlock is) as we are trying to read records from a table that was getting
        updated at the same time.
    </p>

    <h3>The problem</h3>
    <p>
        You are proobably thinking <i>"You should just talk to the DB team and explain the problem so they can change
            the way their script works"</i>. The problem is that we tried to comunicate the problem on numerous
        occations but was apparant that the DB
        team was unwilling to change how their script works. It was apparant for our team that we needed to take the
        things in our hands as the current design was simply unexceptable for a
        <b>production</b> system.
    </p>

    <h3>Solution</h3>
    <p>
        It was becoming clear for our team that we needed to replicate the table we use from the "source" table to our
        own database so we have
        full controll over it and update the data in a smarter way from the "upstream" table from the DB team.

        After a few meetings we extablished the following requirements of the system:

    <ol>
        <li>It should perform both manual and scheduled replication from the source table to our own table in the
            PostgreSQL</li>
        <li>The system should be inexpensive to operate</li>
        <li>The web app should not be affected in any way while the table data is updated from upstream table</li>
    </ol>

    Our team came up with this system design:

    <br>
    </p>

    <br>
    <br>

    <img width="900px" src="/assets/posts/2/system_design.png" alt="System design diagram">


    <p>
        The main idea behind the architecture of the the system is to have a EC2 instance which executes a python script
        at boot time
        that does the data replication
        and when that process is done, the script terminates the instance (i.e. self-terminate). The EC2 instance is
        created/launched by a lambda
        function. We chouse lambda as it allwed us to satisfy the first requirement - the function can be triggered
        manually
        (i.e.
        AWS Console) or on a scheduled basis using
        <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html">CloudWatch Scheduled
            events</a>.

        <br> By using Lambda and terminating the EC2 instance we satisfied the second requirement as we only pay for the
        computing time. That applies to Lambda as well as the EC2 instance as it get's terminated when it finishes its
        work. <br>
        The last requirement is satisfied by the use of 2 database tables created in our Aurora Serverless database. We
        use <a href="https://en.wikipedia.org/wiki/Blue-green_deployment">BLUE/GREEN deployment</a> strategy which
        allows us to perform a sync into a table while our customers are using another table. At the end of the sync we
        swap the "current" table (i.e. the table that is used by our app/customers) with the one that we performed the
        sync
        to.
    </p>

    <h4>Python script workflow</h4>
    <p>
        The workflow of the script can be seen in the following diagram:
        <br><img height="500px" width="600px" src="/assets/posts/2/script_workflow.png" alt="Python script diagram"><br>
        The first thing that the script does is to connect to the source database (in our case that is a MSSQL Server)
        and perform an
        export
        of the source table to a csv file on the EC2 instance itself. If that is successful, we query a DynamoDB table
        that stores a single item which tells us the "current table" name (either TABLE_BLUE & TABLE_GREEN) for example
        TABLE_GREEN. Then, we delete all of the records of the "future current" table - "TABLE_BLUE" and start updating
        it with the data from the csv file.
        When that is finished, we recreate the <b>view</b> using "TABLE_BLUE" table and set
        the "current name" to be "TABLE_BLUE". Last step is to shut down the shutdown the EC2 instance.

        <br>
    </p>

    <h5>main.py</h5>
    <p>
        Bellow you can see the contents of the <code>main</code> method inside main.py. It goes through all of the steps
        definied in the script workflow above.
    <pre class="prettyprint lang-py">
        <code>
    # start inserting records in the DB if there was a successfull export
    if(export_source_db_data()):

        # get the 'CurrentTableName' from the DynamoDB table
        current_table = get_current_table_name()

        if current_table == 'TABLE_BLUE':
            new_table_name = 'TABLE_GREEN'
        else:
            new_table_name = 'TABLE_BLUE'

        # use string interpolation to populate the name of the current table
        insert_sql = insert_sql.format(current_table_name=new_table_name)

        delete_all_from_new_table_sql = "delete from {new_table_name}".format(
            new_table_name=new_table_name)
        # delete all of the records in the 'new_table' before we start inserting records into it
        rds_client.execute_statement(resourceArn=db_cluster_arn, secretArn=db_credentials_secrets_store_arn,
                                        database=aurora_database_name, sql=delete_all_from_new_table_sql)

        parameter_set = []

        with open(fileName, 'r') as file:
            reader = csv.DictReader(file, delimiter=',')

            for row in reader:

                entry = get_entry(row)

                if(len(parameter_set) == batch_size):
                    try_execute_bath_transaction(insert_sql, parameter_set)

                    transaction_count = transaction_count + 1
                    print(
                        f'Transaction count: {transaction_count}. Num of failed transactions: {num_of_failed_transactions}')

                    parameter_set.clear()
                    parameter_set.append(entry)
                else:
                    parameter_set.append(entry)
            # check if we have records that didn't fit into a batch (i.e. less than batch_size)
            if(len(parameter_set) > 0):
                try_execute_bath_transaction(insert_sql, parameter_set)
                transaction_count = transaction_count + 1
                print(f'Transaction count: {transaction_count}')

        # if we have inserted all of the records without an issue
        if(num_of_failed_transactions == 0):
            update_current_table_name(new_table_name)
            update_db_views(new_table_name)
            notify_devs('Full database sync completed successfully! Currently used table: {new_table_name}.\n The sync took: {elapsed_time} minutes'.format(
                new_table_name=new_table_name, elapsed_time=calculate_elapsed_time()))
        else:
            notify_devs('Failed to perform Source database sync.')
    else:
        notify_devs('Failed to export source database.')

    # Self-Terminate the Instance
    subprocess.Popen(self_terminate_bash_commnand.split())
        </code></pre>
    </p>
    <h5>Source code</h5>
    <p>
        If you would like to have a look at the source code of the project, please head to my GitHub repo <a
            href="https://github.com/georgikoemdzhiev/data-sync-aws-py">post</a>
    </p>
</div>